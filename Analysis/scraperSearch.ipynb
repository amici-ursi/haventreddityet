{
 "metadata": {
  "name": "",
  "signature": "sha256:2d15985393dff20d936470124b24758691790c0788a073bd59aefd9b88c93435"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Reddit Scraper"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This scraper is designed for finding descriptions of subreddits from the reddit.com/subreddits pages. I made this scraper because this information does not seem to be available via their API.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Scraping functions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time, pickle, requests\n",
      "import numpy as np\n",
      "\n",
      "#functions for finding respective fields \n",
      "\n",
      "def get_substring(text, pos, start_str, end_str):\n",
      "    i_start = text.find(start_str,pos)\n",
      "    pos = i_start \n",
      "    if i_start < 0: \n",
      "        return pos, 'None'\n",
      "    pos += len(start_str)\n",
      "    i_end = text.find(end_str,pos)\n",
      "    return pos, text[pos:i_end]\n",
      "\n",
      "def get_description(text, pos):\n",
      "    start_str = '<div class=\"usertext-body may-blank-within md-container\"><div class=\"md\">'\n",
      "    end_str = '</div>'\n",
      "    return get_substring(text, pos, start_str, end_str)\n",
      "\n",
      "\n",
      "def get_title(text, pos):\n",
      "    start_str = 'class=\"title\" >/r/'\n",
      "    end_str = '</a>'\n",
      "    return get_substring(text, pos, start_str, end_str)\n",
      "\n",
      "def get_nsubscribers(text,pos):\n",
      "    start_str = 'subscribers</span></span><span class=\"score likes\"><span class=\"number\">'\n",
      "    end_str = '</span>'\n",
      "    return get_substring(text, pos, start_str, end_str)\n",
      "\n",
      "def get_age(text,pos):\n",
      "    start_str = '</span>, a community for '\n",
      "    end_str = '</p>'\n",
      "    return get_substring(text, pos, start_str, end_str)\n",
      "\n",
      "def get_next_url(text,pos, it):\n",
      "    if it ==0:\n",
      "        start_str = '<span class=\"nextprev\">view more:&#32;<a href=\"'        \n",
      "    else:\n",
      "        start_str = '</a><span class=\"separator\"></span><a href=\"'\n",
      "    end_str = '\" rel=\"nofollow next\" >'\n",
      "    return get_substring(text,pos,start_str,end_str)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Starting conditions "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sub_dict = pickle.load(open('../data/sub_dict.pkl','rd'))\n",
      "subr_des = dict()\n",
      "#url = subr_des['current_url']\n",
      "it = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Data collection"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sub in sub_dict:\n",
      "    it += 1\n",
      "    #reddit doesn't like scrapers\n",
      "    url = 'http://www.reddit.com/subreddits/search?q=' + sub\n",
      "    \n",
      "    while True:\n",
      "        try:\n",
      "            text = requests.get(url).text\n",
      "            if text.find('<title>Too Many Requests</title>')<0:\n",
      "                break\n",
      "            randn = np.random.exponential(2)\n",
      "            print 'sleeping for ', randn\n",
      "            time.sleep(randn)\n",
      "        except:\n",
      "            print 'connection error'\n",
      "            randn = np.random.exponential(2)\n",
      "            print 'sleeping for ', randn\n",
      "            time.sleep(randn)\n",
      "        \n",
      "    pos = [0]*4\n",
      "    while pos[3] >= 0:\n",
      "        pos[0], title = get_title(text, pos[0])\n",
      "        pos[1], description = get_description(text, pos[1])\n",
      "        pos[2], nsubscribers = get_nsubscribers(text, pos[2])\n",
      "        pos[3], age = get_age(text, pos[3])\n",
      "        \n",
      "        #check to see if the last title has been found\n",
      "        if  pos[0] < 0:\n",
      "            break\n",
      "            \n",
      "        #If there is no description it will read the next one\n",
      "        if pos[1]>pos[2]:\n",
      "            description =' '\n",
      "            pos[1] = pos[0]\n",
      "            \n",
      "        #save as a hash map for quick access\n",
      "        key = title[:title.find(':')].lower()\n",
      "        subr_des[key] = [title,description,nsubscribers,age]\n",
      "        #subr_des['current_url'] = url\n",
      "    \n",
      "    #dump once a page\n",
      "    pickle.dump(subr_des, open('descriptionSearch_dict.pkl','wd'))\n",
      "        \n",
      "    print \"url\", url \n",
      "    print 'visited: ', it, \" out of \", len(sub_dict)\n",
      "    print \"n subs collected: \", len(subr_des)\n",
      "    \n",
      "    \n",
      "    #reddit doesn't like scrapers\n",
      "    randn = np.random.exponential(2)\n",
      "    time.sleep(randn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sleeping for  0.0428631389825\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.36192427334\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.0589493107955\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.053400028689\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.04772122555\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.3479495664\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.03605645493\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.00618930203941\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.705906853568\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.37508475706\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.28210239231\n",
        "url"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/subreddits/search?q=danganronpa\n",
        "n subs collected:  13  out of  18573\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.492250941556\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.148681101664\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.60228797538\n",
        "url"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/subreddits/search?q=HI_Res\n",
        "n subs collected:  13  out of  18573\n",
        "url"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/subreddits/search?q=ScaryBilbo\n",
        "n subs collected:  14  out of  18573\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.68439454737\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 8.73921663132\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.89916706817\n",
        "url"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/subreddits/search?q=PixelLeague\n",
        "n subs collected:  15  out of  18573\n",
        "url"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/subreddits/search?q=StarWarsEU\n",
        "n subs collected:  16  out of  18573\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.267084412479\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.86549038924\n",
        "url"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/subreddits/search?q=Wahoowa\n",
        "n subs collected:  17  out of  18573\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.338157426418\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.977077975547\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3.05529567695\n",
        "url"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/subreddits/search?q=LanguageTechnology\n",
        "n subs collected:  18  out of  18573\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4.28922110371\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2.11223740726\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.202276086527\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.21000181737\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.323649185389\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.150558264279\n",
        "connection error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sleeping for  0.341802416739\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1.54211790117\n",
        "sleeping for "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5.1691596647\n",
        "connection error"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "sleeping for  0.763354836676\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-6a598967e6e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mrandn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexponential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m'sleeping for '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}